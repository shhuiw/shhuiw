<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>The multiqueue block layer</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="shenghui">

    <!-- Le styles -->
    <link rel="stylesheet" href="/theme/css/bootstrap.min.css" type="text/css" />
    <style type="text/css">
      body {
        padding-top: 60px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }
      .tag-1 {
        font-size: 13pt;
      }
      .tag-2 {
        font-size: 10pt;
      }
      .tag-2 {
        font-size: 8pt;
      }
      .tag-4 {
        font-size: 6pt;
     }
    </style>
    <link href="/theme/css/bootstrap-responsive.min.css" rel="stylesheet">
        <link href="/theme/css/font-awesome.css" rel="stylesheet">

    <link href="/theme/css/pygments.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="/theme/images/favicon.ico">
    <link rel="apple-touch-icon" href="/theme/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/theme/images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/theme/images/apple-touch-icon-114x114.png">

    <link href="/" type="application/atom+xml" rel="alternate" title="shhuiw's Technotes ATOM Feed" />

  </head>

  <body>

    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="/index.html">shhuiw's Technotes </a>
          <div class="nav-collapse">
            <ul class="nav">
                          <li class="divider-vertical"></li>
                  <li >
                    <a href="/category/bing-fa.html">
						<i class="icon-folder-open icon-large"></i>并发
					</a>
                  </li>
                  <li class="active">
                    <a href="/category/block-io.html">
						<i class="icon-folder-open icon-large"></i>Block I/O
					</a>
                  </li>
                  <li >
                    <a href="/category/device-mapper.html">
						<i class="icon-folder-open icon-large"></i>device-mapper
					</a>
                  </li>
                  <li >
                    <a href="/category/fio.html">
						<i class="icon-folder-open icon-large"></i>fio
					</a>
                  </li>
                  <li >
                    <a href="/category/locking.html">
						<i class="icon-folder-open icon-large"></i>locking
					</a>
                  </li>
                  <li >
                    <a href="/category/wei-fen-lei.html">
						<i class="icon-folder-open icon-large"></i>未分类
					</a>
                  </li>
                  <li >
                    <a href="/category/wen-jian-xi-tong.html">
						<i class="icon-folder-open icon-large"></i>文件系统
					</a>
                  </li>

                          <ul class="nav pull-right">
                                <li><a href="/archives.html"><i class="icon-th-list"></i>Archives</a></li>
                          </ul>

            </ul>
            <!--<p class="navbar-text pull-right">Logged in as <a href="#">username</a></p>-->
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row">
        <div class="span9" id="content">
<section id="content">
        <article>
                <header>
                        <h1>
                                <a href=""
                                        rel="bookmark"
                                        title="Permalink to The multiqueue block layer">
                                        The multiqueue block layer
                                </a>
                        </h1>
                </header>
                <div class="entry-content">
                <div class="well">
<footer class="post-info">
<span class="label">Date</span>
<abbr class="published" title="2018-11-30T22:00:00+08:00">
        <i class="icon-calendar"></i>Fri 30 November 2018
</abbr>
<span class="label">By</span>
<a href="/author/jonathan-corbet.html"><i class="icon-user"></i>Jonathan Corbet</a>
<span class="label">Category</span>
<a href="/category/block-io.html"><i class="icon-folder-open"></i>Block I/O</a>.


<span class="label">Tags</span>
	<a href="/tag/multiqueue.html"><i class="icon-tag"></i>multiqueue</a>
</footer><!-- /.post-info -->                </div>
                <p>Jonathan Corbet&emsp;&emsp;June 5, 2013, <a href="https://lwn.net">lwn.net</a></p>
<ul>
<li><a href="https://lwn.net/Articles/552904/">The multiqueue block layer</a></li>
<li><a href="http://kernel.dk/systor13-final18.pdf">Linux Block IO: Introducing Multi-queue SSD Access on Multi-core Systems</a></li>
<li><a href="https://lwn.net/Articles/552911/">null_blk.c</a></li>
</ul>
<p>The kernel's block layer is charged with managing I/O to the system's block ("disk drive") devices. It was designed in an era when a high-performance drive could handle hundreds of I/O operations per second (IOPs); the fact that it tends to fall down with modern devices, capable of handling possibly millions of IOPs, is thus not entirely surprising. It has been known for years that significant changes would need to be made to enable Linux to perform well on fast solid-state devices <strong>高速 SSD 对 block layer 有更高的效率要求</strong>. The shape of those changes is becoming clearer as the multiqueue block layer patch set, primarily the work of Jens Axboe and Shaohua Li, gets closer to being ready for mainline merging.</p>
<p>The basic structure of the block layer has not changed a whole lot since it was described for 2.6.10 in <a href="https://lwn.net/Kernel/LDD3/">Linux Device Drivers</a> . It offers two ways for a block driver to hook into the system, one of which is the  <strong>"request" interface. When run in this mode, the block layer maintains a simple request queue; new I/O requests are submitted to the tail of the queue and the driver receives requests from the head. While requests sit in the queue, the block layer can operate on them in a number of ways: they can be reordered to minimize seek operations, adjacent requests can be coalesced into larger operations, and policies for fairness and bandwidth limits can be applied, for example.</strong></p>
<p><strong>request-based interface 的不足之处</strong> This request queue turns out to be one of the biggest bottlenecks in the entire system. It is protected by a <strong>single lock</strong> which, on a large system, will bounce frequently between the processors. It is a <strong>linked list, a notably cache-unfriendly data structure especially when modifications must be made — as they frequently</strong> are in the block layer. As a result, anybody who is trying to develop a driver for <strong>high-performance storage devices wants to do away 不能满足高性能存储设备的要求</strong> with this request queue and replace it with something better.</p>
<p>The second block driver mode — the <strong>"make request" interface</strong> — allows a driver to do exactly that. It hooks the driver into a <strong>much higher part of the stack, shorting out</strong> the request queue and handing I/O requests <strong>directly</strong> to the driver. This interface was not originally intended for high-performance drivers; instead, it is there <strong>for stacked drivers (the MD RAID implementation, for example) that need to process requests before passing them on to the real, underlying device</strong>. <strong>高效但是适用场景受限</strong> Using it in other situations incurs a substantial cost: all of the other queue processing done by the block layer is lost and must be reimplemented in the driver.</p>
<p>The multiqueue block layer work tries to fix this problem by adding a third mode for drivers to use. In this mode, the request queue is <strong>split into a number of separate</strong> queues: <strong>multiple submission queues and one or more hw dispatch queues</strong></p>
<ul>
<li>
<p><strong>Submission queues are set up on a per-CPU or per-node basis. Each CPU submits I/O operations into its own queue, with no interaction with the other CPUs. Contention for the submission queue lock is thus eliminated (when per-CPU queues are used) or greatly reduced (for per-node queues).</strong></p>
</li>
<li>
<p><strong>One or more hardware dispatch queues simply buffer I/O requests for the driver.</strong></p>
</li>
</ul>
<p>While requests are in the submission queue, they can be operated on by the block layer in the usual manner. SSD 存取数据与传统磁盘不同 <strong>Reordering of requests for locality offers little or no benefit on solid-state devices; indeed, spreading requests out across the device might help with the parallel processing of requests. So reordering will not be done, but coalescing requests will reduce the total number of I/O operations, improving performance somewhat.</strong> Since the submission queues are per-CPU, there is no way to coalesce requests submitted to different queues. With no empirical evidence whatsoever, your editor would guess that adjacent requests are <strong>most likely</strong> to come from the same process and, thus, will automatically find their way into the same submission queue, so the <strong>lack of cross-CPU coalescing is probably not a big problem</strong>.</p>
<p>The block layer will move requests from the submission queues into the hardware queues up to the maximum number specified by the driver. Most current devices will have a single hardware queue, but <strong>high-end devices already support multiple queues to increase parallelism</strong>. On such a device, the entire submission and completion path should be able to run on the same CPU as the process generating the I/O, maximizing cache locality (and, thus, performance). If desired, fairness or bandwidth-cap policies can be applied as requests move to the hardware queues, but there will be an associated performance cost. Given the speed of high-end devices, it may not be worthwhile to try to ensure fairness between users; everybody should be able to get all the I/O bandwidth they can use.</p>
<p>This structure makes the writing of a high-performance block driver (relatively) simple. The driver provides a queue_rq() function to handle incoming requests and calls back to the block layer when requests complete. Those wanting to look at an example of how such a driver would work can see <a href="https://lwn.net/Articles/552911/">null_blk.c</a>  in the new-queue branch of Jens's block repository:</p>
<div class="highlight"><pre><span></span>`git://git.kernel.dk/linux-block.git`
</pre></div>


<p>In the current patch set, the multiqueue mode is offered in addition to the existing two modes, so current drivers will continue to work without change. According to <a href="http://kernel.dk/systor13-final18.pdf">this paper on the multiqueue block layer design [PDF]</a>, the hope is that drivers will migrate over to the multiqueue API, allowing the eventual removal of the request-based mode.</p>
<p>This patch set has been significantly reworked in the last month or so; it has gone from a relatively messy series into something rather cleaner. Merging into the mainline would thus appear to be on the agenda for the near future. Since use of this API is optional, existing drivers should continue to work and this merge could conceivably happen as early as 3.11. But, given that the patch set has not yet been publicly posted to any mailing list and does not appear in linux-next, 3.12 seems like a more likely target. Either way, Linux seems likely to have a much better block layer by the end of the year or so.</p>
<h3>comment</h3>
<h4><a href="https://lwn.net/Articles/553076/">Hypothetical progams might desire cross-CPU coalescing of IO</a></h4>
<p><a href="https://lwn.net/Articles/553076/">Posted Jun 6, 2013 5:38 UTC (Thu) by faramir</a> </p>
<p>Our worthy editor suggests that not coalescing IO requests across CPUs is probably not a big problem. If we restrict ourselves to the <strong>original model of UNIX computation (single process/private memory space)</strong>, I would agree.</p>
<p>If we consider multiple processes with synchronization (perhaps via shared memory), or multi-threaded programs; I'm not so sure. Imagine some kind of processing of file data which can be done a block at a time (say a block based cipher or non-adaptive compression). <strong>A multi-threaded/multi-process version of such a program may in fact be running code on multiple CPUs but reading from/writing to the same files (and therefore generating coalescible IO requests.)</strong> Reads from the input file could come from any of the threads/processes engaged in the task.</p>
<p>In the case of compression; due to variable length output chunks; the writer side will have to be coalesced into a single stream in the program itself in order to put the output in the correct order. <strong>Although that might be done by having a management thread simply inform each compression thread when to write; so the actual write calls might still come from different CPUs.</strong></p>
<p>A block based cipher program could probably use lseek() on multiple fds opened to the same output file to maintain correct ordering from each thread.</p>
<p>In either case, it would appear that coalescing across CPUs would be useful. At least if the actual processing required was negligible relative to IO time. It may be that CPUs aren't fast enough to do this for anything beyond ROT13 encryption or simple RLE compression; so it might not matter for now. But it would seem to be at least be a theoretical issue.</p>
<p><a href="https://lwn.net/Articles/553084/">Posted Jun 6, 2013 7:08 UTC (Thu) by dlang</a></p>
<p>&emsp; but how many people are running such parallel processing of single files?</p>
<p>&emsp; And of those who are doing so, how much do they care if their files get processed one at a time using every CPU for that one file, or many files at a time with each CPU processing a different file (and therefor not needing the combined I/O logic)</p>
<p>&emsp; Yes, there are going to be some, but is it really worth crippling the more common cases to help this rare case?</p>
<p><a href="https://lwn.net/Articles/553088/">Posted Jun 6, 2013 7:28 UTC (Thu) by dlang</a></p>
<p>&emsp; a comment I posted elsewhere is also relevant to this discussion. I'll post a link rather than reposting the longer comment</p>
<p>&emsp; in <a href="https://lwn.net/Articles/553086/">https://lwn.net/Articles/553086/</a>  I talk about how I/O coalescing should only be done when the I/O is busy (among other things)</p>
<p><a href="https://lwn.net/Articles/553113/">Posted Jun 6, 2013 12:06 UTC (Thu) by ejr</a> </p>
<p>&emsp;&emsp; See uses of HDF5 and NetCDF file formats. There are many software systems that store in a single, large file, emulating a file system more aligned with the higher-level semantics. Also, think of databases. Counting a case as rare v. common requires an application area.</p>
<p>&emsp;&emsp; But... Parallel HDF5, etc. interfaces handle some coalescing in the "middleware" layer. They've found that relying on the OS leads to, um, sufficient performance variation across different systems and configurations. Yet another standard parallel I/O layer in user-space could help more than trying to jam the solution into the OS.</p>
<p>&emsp;&emsp; But relying on user-space won't help the case multiqueue is attacking.</p>
<p><a href="https://lwn.net/Articles/553255/">Posted Jun 7, 2013 2:27 UTC (Fri) by dlang</a> </p>
<p>&emsp;&emsp;&emsp; when you have things like databases where many applications are accessing one file, how common is it for the <strong>different applications to be making adjacent writes at the same time? It may happen, but it's not going to be the common case.</strong></p>
<p><a href="https://lwn.net/Articles/553268/">Posted Jun 7, 2013 10:06 UTC (Fri) by ejr</a> </p>
<p>&emsp;&emsp;&emsp;&emsp; <strong>In my world (HPC), having multiple coordinated processes accessing slices of a range <em>is</em> the common case. We have to fight for anything else to have even reasonable performance support. See lustre.</strong></p>
<p>&emsp;&emsp;&emsp;&emsp; <strong>But this case often is better handled outside the OS. There could be different interface where clients post their read buffers and the OS fills them in some hardware-optimal order, but that's been considered for &gt;20 years and has no solution yet.</strong></p>
<p><a href="https://lwn.net/Articles/553323/">Posted Jun 7, 2013 18:18 UTC (Fri) by dlang</a> </p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp; you are processing slices of a range, but are you really writing the adjacent slices at almost exactly the same time from multiple processes? that's what it would take to give the OS the chance to combine the output from the different processes into one write to disk.</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp; Also, in HPC aren't you dealing with many different systems accessing your data over the network rather than multiple processes on one machine?</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp; What we are talking about here is the chance that things running on different CPUs in a single system are generating disk writes that the OS could combine into a single write before sending it to the drives</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp; for reads this isn't as big a deal, <strong>readahead</strong> should go a long way towards making the issue moot.</p>
<p><a href="https://lwn.net/Articles/553133/">Posted Jun 6, 2013 14:22 UTC (Thu) by axboe</a> </p>
<p>&emsp; If you write programs like that and expect IO performance to be stellar, you have a problem. It's already the case that <strong>Linux does not merge (注意是 merge 而不是 reorder) IO from independent threads, unless it just happens to either detect this or if explicitly asked to by sharing an IO context.</strong></p>
<p>&emsp; So in general it's not a huge problem. For "legacy" devices that benefit a lot from merging, we can help them out a little bit. They will typically be single queue anyway, and merging at dispatch time on that queue would be trivial to do.</p>
<p><a href="https://lwn.net/Articles/553134/">Posted Jun 6, 2013 14:38 UTC (Thu) by alankila</a> </p>
<p>&emsp; The algorithms used for data processing on modern disks are quite fast, for instance lzo compression has bandwidth approaching 1 GB/s per core.</p>
<p><a href="https://lwn.net/Articles/554305/">Posted Jun 14, 2013 6:39 UTC (Fri) by kleptog</a> </p>
<p>&emsp; Actually, I'd consider <strong>write combining something to be done by the disk itself. These days with storage arrays having gigabytes of battery backed storage there's no real reason to have the kernel worry about things like write combining. Maybe if it affected the communication overhead, but that can be dealt with in other ways (like parallel submission).</strong></p>
                </div><!-- /.entry-content -->
        </article>
</section>
        </div><!--/span-->

                <div class="span3 well sidebar-nav" id="sidebar">
<ul class="nav nav-list">

<li class="nav-header"><h4><i class="icon-folder-close icon-large"></i>Categories</h4></li>
<li>
<a href="/category/bing-fa.html">
    <i class="icon-folder-open icon-large"></i>并发
</a>
</li>
<li>
<a href="/category/block-io.html">
    <i class="icon-folder-open icon-large"></i>Block I/O
</a>
</li>
<li>
<a href="/category/device-mapper.html">
    <i class="icon-folder-open icon-large"></i>device-mapper
</a>
</li>
<li>
<a href="/category/fio.html">
    <i class="icon-folder-open icon-large"></i>fio
</a>
</li>
<li>
<a href="/category/locking.html">
    <i class="icon-folder-open icon-large"></i>locking
</a>
</li>
<li>
<a href="/category/wei-fen-lei.html">
    <i class="icon-folder-open icon-large"></i>未分类
</a>
</li>
<li>
<a href="/category/wen-jian-xi-tong.html">
    <i class="icon-folder-open icon-large"></i>文件系统
</a>
</li>

<li class="nav-header"><h4><i class="icon-tags icon-large"></i>Tags</h4></li>
<li class="tag-4">
    <a href="/tag/relay.html">
        <i class="icon-tag icon-large"></i>relay
    </a>
</li>
<li class="tag-4">
    <a href="/tag/relayfs.html">
        <i class="icon-tag icon-large"></i>relayfs
    </a>
</li>
<li class="tag-4">
    <a href="/tag/seekwatcher.html">
        <i class="icon-tag icon-large"></i>seekwatcher
    </a>
</li>
<li class="tag-4">
    <a href="/tag/util-linux.html">
        <i class="icon-tag icon-large"></i>util-linux
    </a>
</li>
<li class="tag-2">
    <a href="/tag/device-mapper.html">
        <i class="icon-tag icon-large"></i>device-mapper
    </a>
</li>
<li class="tag-2">
    <a href="/tag/btt.html">
        <i class="icon-tag icon-large"></i>btt
    </a>
</li>
<li class="tag-3">
    <a href="/tag/btrecord.html">
        <i class="icon-tag icon-large"></i>btrecord
    </a>
</li>
<li class="tag-2">
    <a href="/tag/multiqueue.html">
        <i class="icon-tag icon-large"></i>multiqueue
    </a>
</li>
<li class="tag-4">
    <a href="/tag/e2fsprogs.html">
        <i class="icon-tag icon-large"></i>e2fsprogs
    </a>
</li>
<li class="tag-1">
    <a href="/tag/blktrace.html">
        <i class="icon-tag icon-large"></i>blktrace
    </a>
</li>
<li class="tag-4">
    <a href="/tag/sbitmap.html">
        <i class="icon-tag icon-large"></i>sbitmap
    </a>
</li>
<li class="tag-4">
    <a href="/tag/sbitmap_queue.html">
        <i class="icon-tag icon-large"></i>sbitmap_queue
    </a>
</li>
<li class="tag-3">
    <a href="/tag/iostat.html">
        <i class="icon-tag icon-large"></i>iostat
    </a>
</li>
<li class="tag-4">
    <a href="/tag/bno_plot.html">
        <i class="icon-tag icon-large"></i>bno_plot
    </a>
</li>
<li class="tag-3">
    <a href="/tag/bio.html">
        <i class="icon-tag icon-large"></i>bio
    </a>
</li>
<li class="tag-4">
    <a href="/tag/request-tagging.html">
        <i class="icon-tag icon-large"></i>request tagging
    </a>
</li>
<li class="tag-3">
    <a href="/tag/virtualbox.html">
        <i class="icon-tag icon-large"></i>virtualbox
    </a>
</li>
<li class="tag-4">
    <a href="/tag/workqueue.html">
        <i class="icon-tag icon-large"></i>workqueue
    </a>
</li>
<li class="tag-4">
    <a href="/tag/atomic_ops.html">
        <i class="icon-tag icon-large"></i>atomic_ops
    </a>
</li>
<li class="tag-4">
    <a href="/tag/iotop.html">
        <i class="icon-tag icon-large"></i>iotop
    </a>
</li>
<li class="tag-4">
    <a href="/tag/blkiomon.html">
        <i class="icon-tag icon-large"></i>blkiomon
    </a>
</li>
<li class="tag-2">
    <a href="/tag/lockdep.html">
        <i class="icon-tag icon-large"></i>lockdep
    </a>
</li>
<li class="tag-1">
    <a href="/tag/blkparse.html">
        <i class="icon-tag icon-large"></i>blkparse
    </a>
</li>
<li class="tag-4">
    <a href="/tag/single-queue.html">
        <i class="icon-tag icon-large"></i>single queue
    </a>
</li>
<li class="tag-4">
    <a href="/tag/f2fs.html">
        <i class="icon-tag icon-large"></i>f2fs
    </a>
</li>
<li class="tag-4">
    <a href="/tag/lock_stat.html">
        <i class="icon-tag icon-large"></i>lock_stat
    </a>
</li>
<li class="tag-3">
    <a href="/tag/btreplay.html">
        <i class="icon-tag icon-large"></i>btreplay
    </a>
</li>
<li class="tag-4">
    <a href="/tag/verify_blkparse.html">
        <i class="icon-tag icon-large"></i>verify_blkparse
    </a>
</li>
<li class="tag-4">
    <a href="/tag/iowatcher.html">
        <i class="icon-tag icon-large"></i>iowatcher
    </a>
</li>
<li class="tag-2">
    <a href="/tag/fio.html">
        <i class="icon-tag icon-large"></i>fio
    </a>
</li>
<li class="tag-3">
    <a href="/tag/blk-mq.html">
        <i class="icon-tag icon-large"></i>blk-mq
    </a>
</li>
<li class="tag-4">
    <a href="/tag/bvec.html">
        <i class="icon-tag icon-large"></i>bvec
    </a>
</li>
<li class="tag-4">
    <a href="/tag/cfq.html">
        <i class="icon-tag icon-large"></i>CFQ
    </a>
</li>
<li class="tag-4">
    <a href="/tag/bfq.html">
        <i class="icon-tag icon-large"></i>BFQ
    </a>
</li>
<li class="tag-4">
    <a href="/tag/blkrawverify.html">
        <i class="icon-tag icon-large"></i>blkrawverify
    </a>
</li>
<li class="tag-3">
    <a href="/tag/dkms.html">
        <i class="icon-tag icon-large"></i>dkms
    </a>
</li>
<li class="tag-3">
    <a href="/tag/btrace.html">
        <i class="icon-tag icon-large"></i>btrace
    </a>
</li>
<li class="tag-4">
    <a href="/tag/kyber.html">
        <i class="icon-tag icon-large"></i>kyber
    </a>
</li>


</ul>        </div><!--/.well -->

      </div><!--/row-->

      <hr>

      <footer>
        <address id="about">
                Proudly powered by <a href="http://pelican.notmyidea.org/">Pelican <i class="icon-external-link"></i></a>,
                                which takes great advantage of <a href="http://python.org">Python <i class="icon-external-link"></i></a>.
        </address><!-- /#about -->

        <p>The theme is from <a href="http://twitter.github.com/bootstrap/">Bootstrap from Twitter <i class="icon-external-link"></i></a>,
                   and <a href="http://fortawesome.github.com/Font-Awesome/">Font-Awesome <i class="icon-external-link"></i></a>, thanks!</p>
      </footer>

    </div><!--/.fluid-container-->



    <!-- Le javascript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="/theme/js/jquery-1.7.2.min.js"></script>
    <script src="/theme/js/bootstrap.min.js"></script>
  </body>
</html>